{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11f4cdd2-24d5-4cf3-b604-d34adb511a5e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-22T01:03:44.349596Z",
     "iopub.status.busy": "2023-06-22T01:03:44.349304Z",
     "iopub.status.idle": "2023-06-22T01:03:57.587459Z",
     "shell.execute_reply": "2023-06-22T01:03:57.586496Z",
     "shell.execute_reply.started": "2023-06-22T01:03:44.349574Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pycocotools\n",
      "  Downloading pycocotools-2.0.6.tar.gz (24 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.9/dist-packages (from pycocotools) (3.6.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from pycocotools) (1.23.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=2.1.0->pycocotools) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=2.1.0->pycocotools) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=2.1.0->pycocotools) (2.8.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=2.1.0->pycocotools) (9.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=2.1.0->pycocotools) (0.11.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=2.1.0->pycocotools) (23.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=2.1.0->pycocotools) (4.38.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=2.1.0->pycocotools) (1.0.7)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools) (1.14.0)\n",
      "Building wheels for collected packages: pycocotools\n",
      "  Building wheel for pycocotools (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pycocotools: filename=pycocotools-2.0.6-cp39-cp39-linux_x86_64.whl size=400023 sha256=b02e1b122807ff667a354742f079b8571aa35f2c9585a5935c1eac66cda162d4\n",
      "  Stored in directory: /root/.cache/pip/wheels/29/98/97/6c7dca1f8e4c854e15a2676ac98ae3f46ec83ee031d827a5c8\n",
      "Successfully built pycocotools\n",
      "Installing collected packages: pycocotools\n",
      "Successfully installed pycocotools-2.0.6\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install pycocotools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d7af864-16c8-44fa-898a-65b8ae831a8f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-22T01:03:03.464037Z",
     "iopub.status.busy": "2023-06-22T01:03:03.463772Z",
     "iopub.status.idle": "2023-06-22T01:03:06.405711Z",
     "shell.execute_reply": "2023-06-22T01:03:06.405135Z",
     "shell.execute_reply.started": "2023-06-22T01:03:03.464016Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FasterRCNN(\n",
       "  (transform): GeneralizedRCNNTransform(\n",
       "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
       "  )\n",
       "  (backbone): BackboneWithFPN(\n",
       "    (body): IntermediateLayerGetter(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fpn): FeaturePyramidNetwork(\n",
       "      (inner_blocks): ModuleList(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_blocks): ModuleList(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (extra_blocks): LastLevelMaxPool()\n",
       "    )\n",
       "  )\n",
       "  (rpn): RegionProposalNetwork(\n",
       "    (anchor_generator): AnchorGenerator()\n",
       "    (head): RPNHead(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): RoIHeads(\n",
       "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
       "    (box_head): FastRCNNConvFCHead(\n",
       "      (0): Conv2dNormActivation(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (1): Conv2dNormActivation(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Conv2dNormActivation(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Conv2dNormActivation(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): Flatten(start_dim=1, end_dim=-1)\n",
       "      (5): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "      (6): ReLU(inplace=True)\n",
       "    )\n",
       "    (box_predictor): FastRCNNPredictor(\n",
       "      (cls_score): Linear(in_features=1024, out_features=5, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=20, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import LocoFasterRCNN\n",
    "\n",
    "faster_rcnn = LocoFasterRCNN.LocoFasterRCNN()\n",
    "model = faster_rcnn.get_model()\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "# Step 2: Load the state_dict into the model\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ae19c42-103b-4c65-8a22-7b8ac1dd3825",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-22T01:05:40.662942Z",
     "iopub.status.busy": "2023-06-22T01:05:40.662668Z",
     "iopub.status.idle": "2023-06-22T01:05:44.215760Z",
     "shell.execute_reply": "2023-06-22T01:05:44.214926Z",
     "shell.execute_reply.started": "2023-06-22T01:05:40.662921Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=1.81s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.12s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.93s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.13s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.18s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.08s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "import LocoUtils\n",
    "\n",
    "batch_size = 4\n",
    "#load data\n",
    "# Example Usage:\n",
    "base_img_folder = '/notebooks/dataset/loco_new/images'\n",
    "annotations_folder = '/notebooks/dataset/loco_new/annotations'\n",
    "loader = LocoUtils.LocoDatasetLoader(base_img_folder, annotations_folder)\n",
    "train_data_loader = loader.get_train(batch_size=batch_size)\n",
    "val_data_loader = loader.get_val(batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3263d489-3f88-4b26-bfa8-37a9b92fc795",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-22T01:05:49.654053Z",
     "iopub.status.busy": "2023-06-22T01:05:49.653747Z",
     "iopub.status.idle": "2023-06-22T01:05:49.665849Z",
     "shell.execute_reply": "2023-06-22T01:05:49.665153Z",
     "shell.execute_reply.started": "2023-06-22T01:05:49.654032Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "model_folder = \"/notebooks/models/fasterRCNN5/\"\n",
    "\n",
    "# Initialize GradScaler\n",
    "scaler = GradScaler()\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Define the parameters for training\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=learning_rate, momentum=0.9, weight_decay=0.0005)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "    \n",
    "def validate(val_model, val_data):\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_data:\n",
    "            val_imgs = list(img.to(device) for img in images)\n",
    "            with autocast():\n",
    "                labels = [{k: v.to(device) for k, v in t.items()} for t in labels]\n",
    "                val_loss_dict = val_model(val_imgs, labels)\n",
    "                val_losses = sum(loss for loss in val_loss_dict.values())\n",
    "            val_loss += val_losses\n",
    "    val_loss /= len(val_data)\n",
    "    return val_loss\n",
    "\n",
    "\n",
    "def train(train_model, train_data, epoch, accu_steps):\n",
    "    train_model.train()\n",
    "    epoch_loss = 0.0\n",
    "    optimizer.zero_grad()  # Zero the gradients at the beginning of the epoch\n",
    "    \n",
    "    total_batches = len(train_data)\n",
    "    quarter = total_batches // 4  # 25% of total batches\n",
    "    half = total_batches // 2  # 50% of total batches\n",
    "    three_quarters = (3 * total_batches) // 4  # 75% of total batches\n",
    "    \n",
    "    for batch, (imgs, annotations) in enumerate(train_data):\n",
    "        # Move images to device and use AMP for forward pass\n",
    "        imgs = list(img.to(device) for img in imgs)\n",
    "\n",
    "        # Automatic mixed precision\n",
    "        with autocast():\n",
    "            # Move annotations to device\n",
    "            annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]\n",
    "\n",
    "            # Forward pass (compute the loss)\n",
    "            loss_dict = train_model(imgs, annotations)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        # Normalize loss for gradient accumulation\n",
    "        normalized_loss = losses / accu_steps\n",
    "        \n",
    "        # Backward pass and scale the gradients\n",
    "        scaler.scale(normalized_loss).backward()\n",
    "\n",
    "        # Update weights every accumulation_steps\n",
    "        if (batch+1) % accu_steps == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        #print(f'Iteration: {batch}/{len_dataloader}, Loss: {losses}')\n",
    "        \n",
    "        epoch_loss += losses\n",
    "\n",
    "        # Logging\n",
    "        if batch in [quarter, half, three_quarters]:\n",
    "            avg_loss = epoch_loss / (batch + 1)\n",
    "            #validation_loss = validate(model, val_data_loader)\n",
    "            print(f\"Epoch: {epoch} Iteration: {batch}/{total_batches}, Avg Train Loss: {avg_loss}\")\n",
    "\n",
    "    # Complete any remaining accumulation steps\n",
    "    if (total_batches % accu_steps) != 0:\n",
    "        # optimizer step in mixed-precision\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    # Average epoch loss\n",
    "    epoch_loss /= total_batches\n",
    "    validation_loss = validate(train_model, val_data_loader)\n",
    "    print(f\"End of Epoch {epoch}, Avg Train Loss: {epoch_loss}, Avg Val Loss: {validation_loss}\")  \n",
    "    torch.save(model.state_dict(), f\"{model_folder}fasterRCNN_valloss_{validation_loss:.5f}_t_{round(time.time())}_epoch{epoch}.pth\")\n",
    "    \n",
    "    lr_scheduler.step()\n",
    "    \n",
    "    return validation_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62184ec0-bf38-4243-b04e-ff3aa4306ddc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-22T01:06:07.630749Z",
     "iopub.status.busy": "2023-06-22T01:06:07.630309Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Effectiv Batch Size: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Iteration: 176/705, Avg Train Loss: 1.6924943923950195\n",
      "Epoch: 0 Iteration: 352/705, Avg Train Loss: 1.5123544931411743\n",
      "Epoch: 0 Iteration: 528/705, Avg Train Loss: 1.4282758235931396\n",
      "End of Epoch 0, Avg Train Loss: 1.3873661756515503, Avg Val Loss: 1.0961534976959229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [03:22<1:04:09, 202.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Iteration: 176/705, Avg Train Loss: 1.1801424026489258\n",
      "Epoch: 1 Iteration: 352/705, Avg Train Loss: 1.1690423488616943\n",
      "Epoch: 1 Iteration: 528/705, Avg Train Loss: 1.163226842880249\n",
      "End of Epoch 1, Avg Train Loss: 1.1575835943222046, Avg Val Loss: 0.9902628660202026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 2/20 [06:43<1:00:27, 201.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 Iteration: 176/705, Avg Train Loss: 1.0942485332489014\n",
      "Epoch: 2 Iteration: 352/705, Avg Train Loss: 1.0910834074020386\n",
      "Epoch: 2 Iteration: 528/705, Avg Train Loss: 1.089572548866272\n",
      "End of Epoch 2, Avg Train Loss: 1.0835442543029785, Avg Val Loss: 0.9505996704101562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 3/20 [10:03<56:57, 201.03s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 Iteration: 176/705, Avg Train Loss: 1.0660642385482788\n",
      "Epoch: 3 Iteration: 352/705, Avg Train Loss: 1.037660002708435\n",
      "Epoch: 3 Iteration: 528/705, Avg Train Loss: 1.0393742322921753\n",
      "End of Epoch 3, Avg Train Loss: 1.030774712562561, Avg Val Loss: 0.9223023653030396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4/20 [13:24<53:37, 201.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 Iteration: 176/705, Avg Train Loss: 0.988882839679718\n",
      "Epoch: 4 Iteration: 352/705, Avg Train Loss: 0.9942277669906616\n",
      "Epoch: 4 Iteration: 528/705, Avg Train Loss: 0.986353874206543\n",
      "End of Epoch 4, Avg Train Loss: 0.9894009828567505, Avg Val Loss: 0.9119749069213867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 5/20 [16:46<50:16, 201.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 Iteration: 176/705, Avg Train Loss: 0.9650481343269348\n",
      "Epoch: 5 Iteration: 352/705, Avg Train Loss: 0.9720538258552551\n",
      "Epoch: 5 Iteration: 528/705, Avg Train Loss: 0.9621459245681763\n",
      "End of Epoch 5, Avg Train Loss: 0.9602461457252502, Avg Val Loss: 0.8940663933753967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 6/20 [20:07<46:54, 201.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 Iteration: 176/705, Avg Train Loss: 0.9606383442878723\n",
      "Epoch: 6 Iteration: 352/705, Avg Train Loss: 0.9460082650184631\n",
      "Epoch: 6 Iteration: 528/705, Avg Train Loss: 0.935067355632782\n",
      "End of Epoch 6, Avg Train Loss: 0.9298205375671387, Avg Val Loss: 0.8953602313995361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 7/20 [23:27<43:32, 200.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 Iteration: 176/705, Avg Train Loss: 0.8937671780586243\n",
      "Epoch: 7 Iteration: 352/705, Avg Train Loss: 0.9031398892402649\n",
      "Epoch: 7 Iteration: 528/705, Avg Train Loss: 0.9078530669212341\n",
      "End of Epoch 7, Avg Train Loss: 0.9029924869537354, Avg Val Loss: 0.8951126337051392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 8/20 [26:48<40:10, 200.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 Iteration: 176/705, Avg Train Loss: 0.8975557088851929\n",
      "Epoch: 8 Iteration: 352/705, Avg Train Loss: 0.8819948434829712\n",
      "Epoch: 8 Iteration: 528/705, Avg Train Loss: 0.8778542876243591\n",
      "End of Epoch 8, Avg Train Loss: 0.8822071552276611, Avg Val Loss: 0.8915929794311523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 9/20 [30:09<36:50, 200.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 Iteration: 176/705, Avg Train Loss: 0.8790626525878906\n",
      "Epoch: 9 Iteration: 352/705, Avg Train Loss: 0.8701871037483215\n",
      "Epoch: 9 Iteration: 528/705, Avg Train Loss: 0.8591680526733398\n",
      "End of Epoch 9, Avg Train Loss: 0.8616758584976196, Avg Val Loss: 0.8960938453674316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10/20 [33:30<33:30, 201.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 Iteration: 176/705, Avg Train Loss: 0.8309755325317383\n",
      "Epoch: 10 Iteration: 352/705, Avg Train Loss: 0.846805989742279\n",
      "Epoch: 10 Iteration: 528/705, Avg Train Loss: 0.8410441279411316\n",
      "End of Epoch 10, Avg Train Loss: 0.8407310843467712, Avg Val Loss: 0.9018093347549438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 11/20 [36:52<30:09, 201.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11 Iteration: 176/705, Avg Train Loss: 0.8197072148323059\n",
      "Epoch: 11 Iteration: 352/705, Avg Train Loss: 0.8216326236724854\n",
      "Epoch: 11 Iteration: 528/705, Avg Train Loss: 0.8198654055595398\n",
      "End of Epoch 11, Avg Train Loss: 0.8207520246505737, Avg Val Loss: 0.8976460099220276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 12/20 [40:13<26:48, 201.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12 Iteration: 176/705, Avg Train Loss: 0.7865601181983948\n",
      "Epoch: 12 Iteration: 352/705, Avg Train Loss: 0.8013361692428589\n",
      "Epoch: 12 Iteration: 528/705, Avg Train Loss: 0.8081340193748474\n",
      "End of Epoch 12, Avg Train Loss: 0.8024759292602539, Avg Val Loss: 0.9105001091957092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 13/20 [43:34<23:27, 201.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13 Iteration: 176/705, Avg Train Loss: 0.7864357233047485\n",
      "Epoch: 13 Iteration: 352/705, Avg Train Loss: 0.7762001752853394\n",
      "Epoch: 13 Iteration: 528/705, Avg Train Loss: 0.7839877009391785\n",
      "End of Epoch 13, Avg Train Loss: 0.7862030267715454, Avg Val Loss: 0.9178214073181152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 14/20 [46:55<20:06, 201.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14 Iteration: 176/705, Avg Train Loss: 0.7552698850631714\n",
      "Epoch: 14 Iteration: 352/705, Avg Train Loss: 0.7719546556472778\n",
      "Epoch: 14 Iteration: 528/705, Avg Train Loss: 0.7740982174873352\n",
      "End of Epoch 14, Avg Train Loss: 0.7704252004623413, Avg Val Loss: 0.9284406900405884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 15/20 [50:16<16:44, 201.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15 Iteration: 176/705, Avg Train Loss: 0.7240247130393982\n",
      "Epoch: 15 Iteration: 352/705, Avg Train Loss: 0.7463788390159607\n",
      "Epoch: 15 Iteration: 528/705, Avg Train Loss: 0.7494082450866699\n",
      "End of Epoch 15, Avg Train Loss: 0.7528107762336731, Avg Val Loss: 0.9444001317024231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 16/20 [53:36<13:23, 200.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16 Iteration: 176/705, Avg Train Loss: 0.7418927550315857\n",
      "Epoch: 16 Iteration: 352/705, Avg Train Loss: 0.7361294627189636\n",
      "Epoch: 16 Iteration: 528/705, Avg Train Loss: 0.739048182964325\n",
      "End of Epoch 16, Avg Train Loss: 0.7389781475067139, Avg Val Loss: 0.9621924161911011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 17/20 [56:58<10:03, 201.14s/it]"
     ]
    }
   ],
   "source": [
    "accumulation_steps = 4\n",
    "num_epochs = 20\n",
    "\n",
    "print(f\"Effectiv Batch Size: {batch_size*accumulation_steps}\");\n",
    "best_validation = 1000000\n",
    "best_epoch = 0\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    val = train(model, train_data_loader, epoch, accumulation_steps)\n",
    "    if val < best_validation:\n",
    "        best_validation = val\n",
    "        best_epoch = epoch\n",
    "\n",
    "print(best_epoch)\n",
    "print(best_validation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
